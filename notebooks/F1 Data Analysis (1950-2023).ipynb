{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c81f748-fe81-4995-b173-690949fcfd42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSess = SparkSession.builder.appName('F1Analysis').getOrCreate()\n",
    "\n",
    "input_df = sparkSess.read.format('csv').option('header',True).load('dbfs:/FileStore/tables/drivers.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc4f71-0593-4878-933c-38a7b7d44830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./reader_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc89042-bb36-4aea-b982-647c08e8402c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class workFlow:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        driver_data = load_data_source(\n",
    "            data_type = \"csv\",\n",
    "            file_path = 'dbfs:/FileStore/tables/drivers.csv'\n",
    "        ).get_data_frame()\n",
    "\n",
    "        driver_data.show()\n",
    "\n",
    "workflow = workFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e2265c-e65a-48bb-97f1-efa4c34141db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\n|driverId| driverRef|number|code| forename|   surname|       dob|nationality|                 url|\n+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\n|       1|  hamilton|    44| HAM|    Lewis|  Hamilton|1985-01-07|    British|http://en.wikiped...|\n|       2|  heidfeld|    \\N| HEI|     Nick|  Heidfeld|1977-05-10|     German|http://en.wikiped...|\n|       3|   rosberg|     6| ROS|     Nico|   Rosberg|1985-06-27|     German|http://en.wikiped...|\n|       4|    alonso|    14| ALO| Fernando|    Alonso|1981-07-29|    Spanish|http://en.wikiped...|\n|       5|kovalainen|    \\N| KOV|   Heikki|Kovalainen|1981-10-19|    Finnish|http://en.wikiped...|\n|       6|  nakajima|    \\N| NAK|   Kazuki|  Nakajima|1985-01-11|   Japanese|http://en.wikiped...|\n|       7|  bourdais|    \\N| BOU|Sébastien|  Bourdais|1979-02-28|     French|http://en.wikiped...|\n|       8| raikkonen|     7| RAI|     Kimi| Räikkönen|1979-10-17|    Finnish|http://en.wikiped...|\n|       9|    kubica|    88| KUB|   Robert|    Kubica|1984-12-07|     Polish|http://en.wikiped...|\n|      10|     glock|    \\N| GLO|     Timo|     Glock|1982-03-18|     German|http://en.wikiped...|\n|      11|      sato|    \\N| SAT|   Takuma|      Sato|1977-01-28|   Japanese|http://en.wikiped...|\n|      12| piquet_jr|    \\N| PIQ|   Nelson|Piquet Jr.|1985-07-25|  Brazilian|http://en.wikiped...|\n|      13|     massa|    19| MAS|   Felipe|     Massa|1981-04-25|  Brazilian|http://en.wikiped...|\n|      14| coulthard|    \\N| COU|    David| Coulthard|1971-03-27|    British|http://en.wikiped...|\n|      15|    trulli|    \\N| TRU|    Jarno|    Trulli|1974-07-13|    Italian|http://en.wikiped...|\n|      16|     sutil|    99| SUT|   Adrian|     Sutil|1983-01-11|     German|http://en.wikiped...|\n|      17|    webber|    \\N| WEB|     Mark|    Webber|1976-08-27| Australian|http://en.wikiped...|\n|      18|    button|    22| BUT|   Jenson|    Button|1980-01-19|    British|http://en.wikiped...|\n|      19|  davidson|    \\N| DAV|  Anthony|  Davidson|1979-04-18|    British|http://en.wikiped...|\n|      20|    vettel|     5| VET|Sebastian|    Vettel|1987-07-03|     German|http://en.wikiped...|\n+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "input_df = workflow.runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a3ddcdd-ae5c-4c8e-bfd8-dcb6d2225eaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a6646a8-fb46-4f6c-9b94-be160506c14e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c730cc-9768-434f-b2b7-45bfab1c9902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What does Spark do when we run the cell?\n",
    "\n",
    "It builds a logical execution plan and creates Jobs, Stages and Tasks based on the computations.     \n",
    "1. A job in Spark represents some kind of computation and is triggered when an action is called such as 'count()', 'collect()', 'save()'.      \n",
    "2. Each job is broken down into smaller, manageable units called Stages are created when there is shuffling of data or wide transformations such as 'groupBy()' or 'reduceByKey()'.\n",
    "3. Tasks are the smallest unit of work in Spark. It represents the computation performed on a single partition of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38797cbe-dd2d-4888-a874-f6c45cbe9806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "F1 Data Analysis (1950-2023)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
